\section{Code séquentiel}

\par Comme nous allons paralléliser un code séquentiel existant,
nous sommes dépendant du langage de ce code. Le code source fourni 
étant du C, naturellement nous nous dirigerons vers une solution 
de parallélisation MPI/C, que nous avons eu l'occasion de prendre 
en main dans le cadre du module PPAR. Cependant, il reste le choix de la 
version MPI que nous utiliserons.\\

\section{Environnement}

\par L'environnement est un point important à clarifier. Nous sommes dans un environnement particulier ; 
celui fournis par l'ARI ; où notre compte utilisateur est stocké sur un NFS, un système de fichier 
distribué, qui nous garantie le fait de pouvoir se logger sur n'importe quelle machine du réseau 
de l'ARI. En utilisant le fait que nous pouvons accéder à n'importe quel fichier présent sur 
notre session et développer ainsi un programme parallèle à ``pseudo'' mémoire partagée.
Ainsi, lors de l'initialisation du système, chaque noeud  pourrait aller lire les données dans le fichier
 dont l'on souhaite calculer les interactions et ainsi éviter les appels à la fonction 
MPI\_Scatter() qui permet d'initialiser chaque noeud du système avec les corps qu'il va héberger.\\

\par Nous pourrions utiliser des fichiers afin de communiquer entre chaque noeud, mais le NFS ne nous
donne que l'illusion d'un système de fichier local, donc il y aurait quand même deux accès réseau pour 
transférer les données, et il faudrait en plus compter deux accès disque, l'un en écriture, l'autre en lecture.
Ce qui est long par rapport à une émission et réception de message MPI.\\

\par Cependant, ne connaissant pas l'environnement sur lequel les correcteurs exécuteront notre 
programme nous avons fait le choix de fournir un code relativement portable. Ne dépendant donc pas du système
de fichier de l'environnement d'exécution.
Même si pour gagner en performance, on est obligé de jouer avec l'architecture de l'environnement d'exécution.\\


\section{API MPI}
\par La version 1.3 (la version que nous avons utilisé dans nos travaux pratiques)
nous fournissait une API permettant d'abstraire 
l'architecture des noeuds et de communiquer entre eux sans se soucier du réseau. C'est
celle-ci que nous avons utilisé durant les travaux pratiques.\\

\par La version 2.x effectue les mêmes opérations que la version 1.3,
cependant elle permet de gérer un pool de threads et d'effectuer des communications unilatérales.\\

\section{API OPEN\_MP}
\par Afin de paralléliser le code sur chaque noeud en utilisant des threads,
deux choix s'offrent à nous : utiliser l'API POSIX ou l'API OpenMP.\\
La première est complètement gérée à partir de MPI-2.x mais nous serons sans doute
obligés d'utiliser des verrous pour l'accès aux données partagées par les threads.
Alors que la seconde nous abstrait de ce genre de considérations, en permettant 
des déclarations de variables privées, visibles uniquement pour le thread dans lequel elle est déclarée, 
ou publique visible pour tous les threads.\\

\par Nous utiliserons donc l'API OpenMP.\\

\section{La solution}
\par Le ``cluster'' que nous utiliserons pour effectuer nos tests 
est homogène, chaque machine a les mêmes processeurs (la même fréquence et le même nombre de core),
la même quantité de mémoire, et les temps de communications seront sensiblement 
les mêmes entre chaque noeud. Nous effectuerons nos tests dans une des salles 
de l'ARI, les machines étant reliées entre elles par un réseau local nous considèrerons 
les canaux comme fiables et MPI nous en garantie le caractère FIFO.
De plus, comme nous comptons recouvrir les temps
de communications par le calcul et que nous allons utiliser l'API OpenMP 
pour créer des threads, nous n'utiliserons pas les nouvelles fonctionnalités offertes
par MPI-2.x. Nous utiliserons donc la version 1.3 de MPI.\\

