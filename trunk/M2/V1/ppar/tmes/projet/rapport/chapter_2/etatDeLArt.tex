\par Comme nous allons paralléliser un code séquentiel existant,
nous sommes dépendant du langage de ce code. Le code source fourni 
étant du C, naturellement nous nous dirigerons vers une solution 
de parallélisation MPI/C, que nous avons eu l'occasion de prendre 
en main dans le cadre du module PPAR. Cependant, il reste le choix de la 
version MPI que nous utiliserons.\\
\par La version 1.3 nous fournissait une API permettant d'abstraire 
l'architecture des noeuds et de communiquer avec sans ce soucier du reseaux. C'est
celle que nous avons utilisé durant les travaux pratiques.\\
\par La version 2.x effectue les mêmes opérations que la version 1.3,
cependant elle permet de gérer un pool de threads et d'effectuer des communications unilatérales.\\
\par Afin de paralléliser le code sur chaque noeud en utilisant des threads,
deux choix s'offrent à nous : utiliser l'API POSIX ou l'API OpenMP.\\
La première est complètement gérée à partir de MPI-2.x mais nous serons sans doute
obligé d'utiliser des verrous pour l'accès aux données partagées par les threads.
Alors que la seconde nous abstrait de ce genre de considérations, en permettant 
des déclarations de variables privées, visibles uniquement pour le thread dans lequel elle est déclarée, 
ou publique visible pour tout les threads.\\

\par Nous utiliserons donc l'API OpenMP.\\

\par Le $"cluster$" que nous utiliserons pour effectuer nos tests 
est homogène, chaque machine a les mêmes processeurs (la même fréquence et le même nombre de core),
la même quantité de mémoire, et les temps de communications seront sensiblement 
les mêmes entre chaque noeud. Nous effectuerons nos tests dans une des salles 
de l'ARI, les machines étant reliées entre elles par un réseau local nous considèrerons 
les canaux comme fiables et MPI nous en garantie le caractère FIFO.
De plus, comme nous comptons recouvrir les temps
de communications par le calcul et que nous allons utiliser l'API OpenMP 
pour créer des threads, nous n'utiliserons pas les nouvelles fonctionnalités offertes
par MPI-2.x. Nous utiliserons donc la version 1.3 de MPI.\\

\par Il reste cependant un point important à clarifiées. Nous sommes dans un environement particulier ; 
celui fournis par l'ARI ; où notre compte utilisateur est stocké sur un NFS, un systeme de fichier 
distribué qui nous garantie le fait de pouvoir se logger sur n'importe quelle machine du réseau 
de l'ARI. En utilisant le faite que nous pouvons accéder à n'importe quel fichier présent sur 
notre session et développer ainsi un programme parallèle à pseudo mémoire partagée.
Ainsi lors de l'initialisation du système, chaque noeud  pourrait aller lire dans le fichier 
que dont l'on souhaite calculer les interactions et ainsi éviter les tout les appels à la fonction 
MPI\_Scatter() qui permet d'initialiser chaque noeud du système avec les corps qu'il va hébergé.\\
Cependant ne connaissant pas l'environement sur lequel les correcteurs éxécuterons notre 
programme nous avons fait le choix d'utiliser l'initialisation par appel à la fonction MPI\_Scatter().\\
De plus nous pourrions utiliser des fichiers afin de communiquer entre chaque noeud, mais le NFS ne nous
donne que l'illusion du systeme de fichier local, donc il y aurait quand même deux accès réseau pour 
transférer les données, et il faudrait en plus compter deux accès disques l'un en écriture l'autre en lecture.
Ce qui est long par raport à une emission et réception de message MPI.\\
